{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901eb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from QuantConnect.Research import QuantBook\n",
    "from QuantConnect import Resolution\n",
    "\n",
    "qb = QuantBook()\n",
    "\n",
    "EXTENDED_HOURS = True\n",
    "\n",
    "# Add SPY minute data\n",
    "spy = qb.AddEquity(\"SPY\", Resolution.Minute, extendedMarketHours=EXTENDED_HOURS).Symbol\n",
    "\n",
    "start = pd.Timestamp(\"2014-12-30 04:00\", tz=\"America/New_York\")\n",
    "end   = pd.Timestamp(\"2025-06-03 20:00\", tz=\"America/New_York\")\n",
    "\n",
    "hist = qb.History(spy, start, end, Resolution.DAILY)\n",
    "\n",
    "if isinstance(hist.index, pd.MultiIndex):\n",
    "    df = hist.loc[spy].reset_index()\n",
    "else:\n",
    "    df = hist.reset_index()\n",
    "\n",
    "# Convert time to UTC if needed\n",
    "if 'time' in df.columns:\n",
    "    df['time'] = pd.to_datetime(df['time'], utc=True)#.dt.tz_convert('America/New_York')\n",
    "\n",
    "#mask = (df['time'].dt.time >= time(9, 0)) & (df['time'].dt.time <= time(10, 0))\n",
    "#df = df.loc[mask, ['time', 'open', 'high', 'low', 'close', 'volume']].copy()\n",
    "\n",
    "# Print CSV text to output\n",
    "csv_text = df.to_csv(index=False)\n",
    "print(csv_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add instruments for factors\n",
    "USO    = qb.AddEquity(\"USO\", Resolution.Daily).Symbol      # Oil ETF\n",
    "GLD    = qb.AddEquity(\"GLD\", Resolution.Daily).Symbol      # Gold ETF\n",
    "EURUSD = qb.AddForex(\"EURUSD\", Resolution.Daily).Symbol    # Euro / USD\n",
    "USDJPY = qb.AddForex(\"USDJPY\", Resolution.Daily).Symbol    # USD / JPY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70248538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all symbols\n",
    "symbols = [spy, USO, GLD, EURUSD, USDJPY]\n",
    "\n",
    "# Pull daily history\n",
    "hist = qb.History(symbols, start, end, Resolution.Daily)\n",
    "\n",
    "# Extract close + volume to wide tables\n",
    "prices = hist.close.unstack(0).sort_index()\n",
    "volumes = hist.volume.unstack(0).sort_index() if \"volume\" in hist.columns else pd.DataFrame(index=prices.index)\n",
    "\n",
    "# Rename columns to string\n",
    "prices.columns = [str(c) for c in prices.columns]\n",
    "volumes.columns = [str(c) for c in volumes.columns]\n",
    "\n",
    "print(\"✅ Data pulled for:\", list(prices.columns))\n",
    "print(prices.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f55f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ny_date(df):\n",
    "    \"\"\"Convert timestamps to NY trading date (00:00 of that date) and take last value per day.\"\"\"\n",
    "    idx = df.index\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize(\"UTC\")\n",
    "    idx = idx.tz_convert(\"America/New_York\").normalize()\n",
    "    df2 = df.copy()\n",
    "    df2.index = idx\n",
    "    return df2.groupby(df2.index).last()\n",
    "\n",
    "# Convert to NY-date index\n",
    "prices_d  = to_ny_date(prices)\n",
    "volumes_d = to_ny_date(volumes) if not volumes.empty else pd.DataFrame(index=prices_d.index)\n",
    "\n",
    "# Reference calendar = SPY trading days\n",
    "ref_dates = prices_d[\"SPY\"].dropna().index\n",
    "\n",
    "# Reindex to SPY calendar\n",
    "prices_d  = prices_d.reindex(ref_dates)\n",
    "volumes_d = volumes_d.reindex(ref_dates)\n",
    "\n",
    "# Fill missing data for non-SPY assets\n",
    "for c in [\"USO\", \"GLD\", \"EURUSD\", \"USDJPY\"]:\n",
    "    if c in prices_d.columns:\n",
    "        prices_d[c] = prices_d[c].ffill().bfill()\n",
    "\n",
    "# Fill SPY volume if needed\n",
    "if \"SPY\" in volumes_d.columns:\n",
    "    volumes_d[\"SPY\"] = volumes_d[\"SPY\"].ffill().bfill()\n",
    "\n",
    "print(\"Aligned rows:\", len(prices_d))\n",
    "print(\"NaNs by column:\\n\", prices_d[[\"SPY\",\"USO\",\"GLD\",\"EURUSD\",\"USDJPY\"]].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97698e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.DataFrame(index=prices_d.index)\n",
    "\n",
    "# === SPY Flows ===\n",
    "spy_close = prices_d[\"SPY\"]\n",
    "spy_vol   = volumes_d[\"SPY\"]\n",
    "\n",
    "feat[\"flow_vol_z20\"]       = (spy_vol - spy_vol.rolling(20).mean()) / spy_vol.rolling(20).std()\n",
    "feat[\"flow_money_proxy5\"]  = (spy_close * spy_vol).pct_change(5)\n",
    "\n",
    "# === Commodities ===\n",
    "feat[\"uso_ret5\"] = prices_d[\"USO\"].pct_change(5) if \"USO\" in prices_d.columns else np.nan\n",
    "feat[\"gld_ret5\"] = prices_d[\"GLD\"].pct_change(5) if \"GLD\" in prices_d.columns else np.nan\n",
    "\n",
    "# === FX ===\n",
    "feat[\"eurusd_ret5\"] = prices_d[\"EURUSD\"].pct_change(5) if \"EURUSD\" in prices_d.columns else np.nan\n",
    "feat[\"usdjpy_ret5\"] = prices_d[\"USDJPY\"].pct_change(5) if \"USDJPY\" in prices_d.columns else np.nan\n",
    "feat[\"usd_strength\"] = 0.5 * (-feat[\"eurusd_ret5\"]) + 0.5 * (feat[\"usdjpy_ret5\"])\n",
    "\n",
    "# Drop initial rolling window\n",
    "MAX_LOOKBACK = 20\n",
    "feat = feat.iloc[MAX_LOOKBACK:].replace([np.inf, -np.inf], np.nan).dropna(how=\"any\")\n",
    "\n",
    "print(\"Features created:\", feat.shape)\n",
    "feat.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"spy_ohlcv.csv\"):\n",
    "    print(\"✅ spy_ohlcv.csv already exists.\")\n",
    "else:\n",
    "    try:\n",
    "        df.to_csv(\"spy_ohlcv.csv\", index=False)\n",
    "        print(\"✅ wrote spy_ohlcv.csv from existing df:\", df.shape)\n",
    "    except NameError:\n",
    "        hist_spy = qb.History(spy, start, end, Resolution.Daily)\n",
    "        if isinstance(hist_spy.index, pd.MultiIndex):\n",
    "            df1_tmp = hist_spy.loc[spy].reset_index()\n",
    "        else:\n",
    "            df1_tmp = hist_spy.reset_index()\n",
    "        # Normalize time column to UTC tz-aware\n",
    "        if 'time' in df1_tmp.columns:\n",
    "            df1_tmp['time'] = pd.to_datetime(df1_tmp['time'], utc=True)\n",
    "        # Keep standard columns\n",
    "        keep = [c for c in ['time','open','high','low','close','volume'] if c in df1_tmp.columns]\n",
    "        df1_tmp = df1_tmp[keep].copy()\n",
    "        df1_tmp.to_csv(\"spy_ohlcv.csv\", index=False)\n",
    "        print(\"✅ rebuilt & wrote spy_ohlcv.csv:\", df1_tmp.shape)\n",
    "\n",
    "df1_check = pd.read_csv(\"spy_ohlcv.csv\", parse_dates=[\"time\"])\n",
    "print(\"spy_ohlcv.csv head:\\n\", df1_check.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a838f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"spy_ohlcv.csv\", parse_dates=[\"time\"])\n",
    "\n",
    "# Convert SPY timestamps -> NY date (no tz)\n",
    "t1 = pd.to_datetime(df1[\"time\"], utc=True)\n",
    "df1[\"ny_date\"] = t1.dt.tz_convert(\"America/New_York\").dt.date\n",
    "\n",
    "# Right side: your features 'feat' (index is NY-date-like)\n",
    "feat_out = feat.copy()\n",
    "\n",
    "idx = feat_out.index\n",
    "# If index is tz-aware, convert to NY; else just coerce to datetime then .date\n",
    "if getattr(idx, \"tz\", None) is not None:\n",
    "    ny_dates = idx.tz_convert(\"America/New_York\").date\n",
    "else:\n",
    "    ny_dates = pd.to_datetime(idx).date\n",
    "\n",
    "feat_out = feat_out.reset_index(drop=True)\n",
    "feat_out[\"ny_date\"] = ny_dates\n",
    "\n",
    "# Merge on the DATE key\n",
    "merged = pd.merge(df1, feat_out, on=\"ny_date\", how=\"inner\").drop(columns=[\"ny_date\"])\n",
    "\n",
    "# Save & preview\n",
    "merged.to_csv(\"spy_features_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e42488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuantConnect import Resolution\n",
    "\n",
    "try:\n",
    "    RSP = qb.AddEquity(\"RSP\", Resolution.Daily).Symbol\n",
    "    rsp_hist = qb.History(RSP, start, end, Resolution.Daily)\n",
    "\n",
    "    # Convert to wide close series\n",
    "    if isinstance(rsp_hist.index, pd.MultiIndex):\n",
    "        rsp_close_raw = rsp_hist.close.unstack(0)[\"RSP\"]\n",
    "    else:\n",
    "        rsp_close_raw = rsp_hist.set_index(\"time\")[\"close\"]\n",
    "\n",
    "    # Reuse your to_ny_date() helper from earlier cell\n",
    "    rsp_close_d = to_ny_date(rsp_close_raw.to_frame(name=\"RSP\"))[\"RSP\"]\n",
    "\n",
    "    # Align to SPY trading calendar and fill around holidays\n",
    "    rsp_close_d = rsp_close_d.reindex(prices_d.index).ffill().bfill()\n",
    "\n",
    "    print(\"✅ RSP aligned:\", rsp_close_d.index.min(), \"→\", rsp_close_d.index.max(), \"| NaNs:\", rsp_close_d.isna().sum())\n",
    "    has_rsp = True\n",
    "except Exception as e:\n",
    "    print(\"⚠️ RSP not available, skipping breadth proxy. Reason:\", e)\n",
    "    has_rsp = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92756850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Breadth proxy: equal-weight outperforming cap-weight (RSP vs SPY) ===\n",
    "if has_rsp:\n",
    "    spy_close = prices_d[\"SPY\"]\n",
    "\n",
    "    rsp_ret5 = rsp_close_d.pct_change(5)\n",
    "    spy_ret5 = spy_close.pct_change(5)\n",
    "\n",
    "    feat[\"breadth_proxy\"] = (rsp_ret5 - spy_ret5)\n",
    "\n",
    "    # Clean only new column's initial lookback; keep prior cleaning\n",
    "    feat = feat.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    feat = feat.dropna(subset=[\"breadth_proxy\"])\n",
    "\n",
    "    print(\"✅ Added breadth_proxy. Feature shape now:\", feat.shape)\n",
    "else:\n",
    "    print(\"↩️ Proceeding without breadth_proxy. (Everything else unchanged.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e19a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save updated features ===\n",
    "feat.to_csv(\"spy_features_step2.csv\")\n",
    "print(\"Saved spy_features_step2.csv:\", feat.shape)\n",
    "\n",
    "df1 = pd.read_csv(\"spy_ohlcv.csv\", parse_dates=[\"time\"])\n",
    "t1  = pd.to_datetime(df1[\"time\"], utc=True)\n",
    "df1[\"ny_date\"] = t1.dt.tz_convert(\"America/New_York\").dt.date\n",
    "\n",
    "# Build same NY-date key for features\n",
    "feat_out = feat.copy()\n",
    "idx = feat_out.index\n",
    "if getattr(idx, \"tz\", None) is not None:\n",
    "    ny_dates = idx.tz_convert(\"America/New_York\").date\n",
    "else:\n",
    "    ny_dates = pd.to_datetime(idx).date\n",
    "feat_out = feat_out.reset_index(drop=True)\n",
    "feat_out[\"ny_date\"] = ny_dates\n",
    "\n",
    "merged = pd.merge(df1, feat_out, on=\"ny_date\", how=\"inner\").drop(columns=[\"ny_date\"])\n",
    "\n",
    "out_name = \"spy_features_full_v2.csv\" if \"breadth_proxy\" in feat.columns else \"spy_features_full.csv\"\n",
    "merged.to_csv(out_name, index=False)\n",
    "\n",
    "print(f\"Final merged file: {out_name} | shape:\", merged.shape)\n",
    "print(\"Columns:\", list(merged.columns))\n",
    "\n",
    "print(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.to_csv(\"spy_features_step2.csv\")\n",
    "print(\"Saved spy_features_step2.csv:\", feat.shape)\n",
    "\n",
    "df1 = pd.read_csv(\"spy_ohlcv.csv\", parse_dates=[\"time\"])\n",
    "t1  = pd.to_datetime(df1[\"time\"], utc=True)\n",
    "df1[\"ny_date\"] = t1.dt.tz_convert(\"America/New_York\").dt.date\n",
    "\n",
    "# Build same NY-date key for features\n",
    "feat_out = feat.copy()\n",
    "idx = feat_out.index\n",
    "if getattr(idx, \"tz\", None) is not None:\n",
    "    ny_dates = idx.tz_convert(\"America/New_York\").date\n",
    "else:\n",
    "    ny_dates = pd.to_datetime(idx).date\n",
    "feat_out = feat_out.reset_index(drop=True)\n",
    "feat_out[\"ny_date\"] = ny_dates\n",
    "\n",
    "# Merge everything\n",
    "merged = pd.merge(df1, feat_out, on=\"ny_date\", how=\"inner\").drop(columns=[\"ny_date\"])\n",
    "\n",
    "# === Print full CSV to output ===\n",
    "csv_text = merged.to_csv(index=False)\n",
    "print(csv_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdst3612",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
